use_gpu: true 
gpu_num: -1 #-1 means use all (DP mode)

verbose: true 
debug: false #true won't save anything
save_imgs: true
save_dir: "/content/exp" #root of folder to save run information

seed: 2022

use_autograd: true #whether to use autograd for loss calculateions (vs explicitly forming the function)

#TODO implement these
resume: false 
resume_dir: "" #the root of the checkpoint folder to resume from

net:
 model: 'ncsnv2' 
 config_file: "/content/Inverse_Meta/ncsnv2/configs/ffhq.yml"
 checkpoint_dir: "/content/meta_exp/checkpoints/ffhq/checkpoint_80000.pth" 
 
data:
 dataset: 'ffhq'
 data_path: "/content/meta_exp/datasets/ffhq" 

 num_train: 4
 num_val: 2
 num_test: 8

 train_batch_size: 4
 val_batch_size: 2
 test_batch_size: 4

 image_size: 256
 num_channels: 3

 random_flip: true

outer:
 meta_type: 'mle' #only mle supported

 meta_loss_type: 'l2'

 hyperparam_type: 'vector' #type of hyperparam c to use. [scalar, vector] 
 exp_params: false #whether to use exp(hyperparams) instead of hyperparams
 hyperparam_init: 0

 reg_hyperparam: true 
 reg_hyperparam_type: 'l1' #[l1, soft (i.e. ISTA), hard (i.e. hard thresholding)]
 reg_hyperparam_scale: 1 #[l1: float (i.e. reg hyperparm in meta loss), soft: float (i.e. ISTA threshold param), hard: float (i.e. sparsity level)]

 ROI_loss: false
 ROI: None #[None, ((vertical offset, horizontal offset)(height, width))]

opt:
  num_iters: 50 #number of meta iterations to run
  optimizer: 'adam' #[adam, sgd]
  lr: 0.1 #meta learning rate
  batches_per_iter: 1 #number of training batches to use per meta update. -1 means all batches

  decay: false 
  lr_decay: 0.95 #exponential weight decay rate.
  decay_on_val: true #only relevant if decay=True

  val_iters: 10 #validate every n iterations
  checkpoint_iters: 1 #checkpoint every n iterations

inner:
 alg: 'map' #type of inner algorithm. ['map', 'langevin']
 T: 3 #number of inner optimization steps. For langevin, number of steps per noise level. (5 for celeba, 3 for ffhq)

 renormalize: True #whether to re-normalize the likelihood grad
 rescale_factor: 5 

 decimate: false 
 decimation_factor: 1 #factor to divide the number of noise levels by. Only relevant if decimate=True
 decimation_type: 'linear' #['linear', 'log_last', 'log_first', 'last', 'first']

 verbose: 200 #print every n iterations. 0 means don't print. Only prints if verbose=true. 
 lr: 0.0000009 #learning rate. (0.0000033 for celeba, 0.0000009 for ffhq for ncsnv2)

problem:
 measurement_type: 'fourier' #measurement type ['superres', 'inpaint', 'identity', 'gaussian', 'fourier']
 learn_samples: true #whether the problem is one of learning samples; not usable for gaussian or identity
 sample_pattern: 'random' #the type of pattern to sample [horizontal, vertical, random]

 num_measurements: 65536 #number of [measurements for gaussian, kept pixel locations for fourier/inpainting]
 fourier_mask_type: 'random' #[radial, horizontal, vertical, random] pattern for Fourier subsampling mask
 #downsample_factor: 16 #factor to downsample each side of the image
 #inpaint_size: 20 #masked region side length for inpainting. Only relevant if inpaint_random=false
 #inpaint_random: false #whether to use random inpainting instead of a centered box. Uses num_measurements if true.  
 #efficient_inp: true #use binary mask instead of A matrix to get inpaint measurements. Only relevant when measurement_type='inpaint'
 
 add_noise: false #whether to add noise after measuring
 noise_type: "gaussian_nonwhite" #[gaussian, gaussian_nonwhite] 
 noise_std: 0.1 
 
 add_dependent_noise: false #whether to add noise before measuring
 dependent_noise_type: "gaussian" #[gaussian, uniform]
 dependent_noise_std: 1