use_gpu: true #whether to use cuda
gpu_num: -1 #the GPU number to use. default is 0. -1 means use all
save_dir: "/content/exp" #root of folder to save run information (might look something like save_dir/exp1, save_dir/exp2, etc.)
seed: 2022
resume: false #whether to resume from a checkpoint
resume_dir: "" #the root of the checkpoint (log) folder to resume from

net:
 model: 'ncsnv2'
 config_file: "/content/Inverse_Meta/ncsnv2/configs/celeba.yml" #Path to a config file for the model (if it used configs)
 checkpoint_dir: "/content/meta_exp/checkpoints/celeba/checkpoint_210000.pth" #Path to pretrained model

data:
 data_path: "/content/meta_exp/datasets/celeba" 
 dataset: 'celeba'
 train_batch_size: 16
 val_batch_size: 16
 num_channels: 3
 num_train: 16
 num_val: 16
 num_test: 64

outer:
 verbose: true #whether to print during execution. 
 plot_imgs: true #whether to plot the data and reconstructions inline. set false if not in a notebook. 
 debug: false #debug mode doesn't save anything. 
 measurement_loss: false #whether the meta-loss should be on held-out measurements instead of true images
 meta_loss_type: 'l2' #what type of meta loss to use
 meta_type: 'mle' #implicit, maml, mle
 maml_use_last: -1 #use only the last n iterations of inner optimization for maml. -1 means use all.
 cg_iters: 25 #number of conjugate gradient iterations to run if using meta_type=implicit. 0 returns b as the solution. A usual value is 25.
 cg_verbose: 5 #print from conjugate gradient every n iterations. 0 means don't print
 cg_tol: 0.00001 #tolerance parameter for conjugate gradient residual. A usual value is 0.00001.
 cg_damping: 1 #damping parameter for conjugate gradient. Turns problem from Ax = b --> (cg_damping*I + A)x = b. Conditions Hessian. 0 means no damping
 use_autograd: true #whether to use autograd for the cond-log-likelihood and meta gradient computation w.r.t. x.
 save_inits: true #whether to save the last found optimal solution for a trainig image to initialize the next inner loop.   
 hyperparam_type: 'vector' #type of hyperparam c to use. [scalar, vector, matrix] 
 exp_params: false #whether to use exp(hyperparams) instead of hyperparams
 hyperparam_init: 0 #the value to initialize the hyperparameter at. Default is 1
 reg_hyperparam: 'l1' #whether to use l1 regularization on c
 reg_hyperparam_scale: 1 #the hyperparameter for regularizing the hyperparameters
 num_iters: 25 #number of meta iterations to run
 optimizer: 'adam' #meta optimizer. [adam, sgd]
 lr: 0.1 #meta learning rate
 lr_decay: 0.95 #exponential weight decay applied if validation loss is rising. -1 or false means no decay
 use_validation: true #if set to true, will use a validation set. 
 decay_on_val: false
 val_iters: 4 #validate every n iterations
 checkpoint_iters: 1 #checkpoint every n iterations
 batches_per_iter: -1 #number of training batches to use per iteration. -1 means all batches 
 finite_difference: false #whether to use finite difference for Hessian-vector product calculations. Automatically true if meta_type=hessian-free  
 finite_difference_coeff: 0.00000001 #value of r to use in finite difference. 0.00000001 is usual val
 ROI: false #[False, True, ((h_offset, w_offset),(h, w))] - whether to use an ROI

inner:
 alg: 'map' #type of inner algorithm. ['map', 'langevin']
 T: 5 #number of inner optimization steps. For langevin, number of steps per noise level. (5 for celeba, 3 for ffhq)
 decimation_factor: 5 #for langevin, factor to divide the number of noise levels by. 0 means don't decimate.
 decimation_type: 'linear' #['linear', 'log_last', 'log_first', 'last', 'first']
 verbose: 100 #print every n iterations. 0 means don't print
 lr: 0.0000033 #learning rate. (0.0000033 for celeba, 0.0000009 for ffhq for ncsnv2)

problem:
 measurement_type: 'gaussian' #measurement type ['superres', 'inpaint', 'identity', 'gaussian', 'circulant']
 num_measurements: 12288 #number of measurements for gaussian and circulant
 downsample_factor: 16 #factor to downsample each side of the image
 inpaint_size: 20 #masked region side length for inpainting
 add_noise: false #whether to add noise after measuring
 noise_type: "gaussian_nonwhite" #[gaussian, gaussian_nonwhite] 
 noise_std: 0.1 
 add_dependent_noise: false #whether to add noise before measuring
 dependent_noise_type: "gaussian" #[gaussian, uniform]
 dependent_noise_std: 1