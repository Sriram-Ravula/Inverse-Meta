meta_type: 'implicit' #implicit, maml, fomaml, hessian-free MAML, 
use_gpu: true
save_dir: ""
seed: 2022
net: 'ncsnv2'

data:
 dataset: 'ffhq'
 train_batch_size: 16
 val_batch_size: 16
 num_channels: 3
 num_train: 64
 num_val: 32
 num_test: 32

outer:
 hyperparam_type: 'vector'
 num_epochs: 100
 optimizer: 'adam'
 lr: 0.01
 lr_decay: 0.95
 verbose: true
 debug: false
 checkpoint_epochs: 2
 cg_iters: 25
 cg_verbose: 5
 cg_tol: 0.00001
 cg_damping: 0.1
 finite_difference: false
 finite_difference_coeff: 0.00000001
 measurement_loss: false
 train_loss_type: 'l2'
 ROI: true #[False, True, ((h_offset, w_offset),(h, w))] - whether to use an ROI

inner:
 alg: 'map' #['map', 'langevin']
 T: 3 #[int] (5 for celeba, 3 for ffhq)
 decimation_factor: 5
 decimation_type: 'linear'
 verbose: 100
 lr: 0.0000009 #[float] (0.0000033 for celeba, 0.0000009 for ffhq)
 num_batches_update: 1 #number of training batches to use before updating weights. -1 means all batches 

problem:
 measurement_type: 'superres'
 num_measurements: 2
 downsample_factor: 4
 inpaint_size: 20
 add_noise: false
 noise_type: "gaussian"
 noise_std: 1
 add_dependent_noise: false
 dependent_noise_type: "gaussian"
 dependent_noise_std: 1
