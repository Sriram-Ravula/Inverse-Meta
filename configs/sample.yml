use_gpu: true #whether to use cuda
save_dir: "" #root of folder to save run information 
seed: 2022
resume: false #whether to resume from a checkpoint
resume_dir: "" #the root of the checkpoint folder to resume from

net:
 model: 'ncsnv2'
 config_file: "" #Path to a config file for the model (if it used configs)
 checkpoint_dir: "" #Path to pretrained model

data:
 data_path: "" #directory to grab the dataset from
 dataset: 'ffhq'
 train_batch_size: 4
 val_batch_size: 4
 num_channels: 3
 num_train: 64
 num_val: 32
 num_test: 32

outer:
 meta_type: 'implicit' #implicit, maml, fomaml, hessian-free MAML, 
 measurement_loss: false #whether the meta-loss should be on held-out measurements instead of true images
 meta_loss_type: 'l2' #what type of meta loss to use
 hyperparam_type: 'vector' #type of hyperparam c to use. [scalar, vector, matrix] 
 num_iters: 100 #number of meta iterations to run
 optimizer: 'adam' #meta optimizer. [adam, sgd]
 lr: 0.01 #meta learning rate
 lr_decay: 0.95 #exponential weight decay applied if validation loss is rising. -1 means no decay
 verbose: true #whether to print during execution. TODO supplant this with command line argument.
 debug: false #debug mode doesn't save anything
 val_iters: 2 #validate every n iterations
 checkpoint_iters: 5 #checkpoint every n iterations
 batches_per_iter: 1 #number of training batches to use per iteration. -1 means all batches 
 cg_iters: 0 #number of conjugate gradient iterations to run if using meta_type=implicit. 0 returns b as the solution. A usual value is 25.
 cg_verbose: 0 #print from conjugate gradient every n iterations. 0 means don't print
 cg_tol: 0.00001 #tolerance parameter for conjugate gradient residual. A usual value is 0.00001.
 cg_damping: 1 #damping parameter for conjugate gradient. Turns problem from Ax = b --> (cg_damping*I + A)x = b. Conditions Hessian. 
 finite_difference: false #whether to use finite difference for Hessian-vector product calculations. Automatically true if meta_type=hessian-free  
 finite_difference_coeff: 0.00000001 #value of r to use in finite difference. 0.00000001 is usual val
 ROI: true #[False, True, ((h_offset, w_offset),(h, w))] - whether to use an ROI

inner:
 alg: 'map' #type of inner algorithm. ['map', 'langevin']
 T: 3 #number of inner optimization steps. For langevin, number of steps per noise level. (5 for celeba, 3 for ffhq)
 decimation_factor: 5 #for langevin, factor to divide the number of noise levels by. 0 means don't decimate.
 decimation_type: 'linear' #['linear', 'log_last', 'log_first', 'last', 'first']
 verbose: 250 #print every n iterations.
 lr: 0.0000009 #learning rate. (0.0000033 for celeba, 0.0000009 for ffhq for ncsnv2)

problem:
 measurement_type: 'superres' #measurement type ['superres', 'inpaint', 'identity', 'gaussian', 'circulant']
 num_measurements: 5000 #number of measurements for gaussian and circulant
 downsample_factor: 8 #factor to downsample each side of the image
 inpaint_size: 20 #masked region side length for inpainting
 add_noise: false #whether to add noise after measuring
 noise_type: "gaussian" #[gaussian, uniform] 
 noise_std: 1 
 add_dependent_noise: false #whether to add noise before measuring
 dependent_noise_type: "gaussian" #[gaussian, uniform]
 dependent_noise_std: 1
