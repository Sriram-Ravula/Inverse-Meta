use_gpu: true #whether to use cuda
gpu_num: 0 #the GPU number to use. default is 0. 
save_dir: "" #root of folder to save run information (format: save_dir/datasets, save_dir/logs, save_dir/checkpoints)
seed: 2022
resume: false #whether to resume from a checkpoint
resume_dir: "" #the root of the checkpoint (log) folder to resume from

net:
 model: 'ncsnv2'
 config_file: "/content/Inverse_Meta/ncsnv2/configs/celeba.yml" #Path to a config file for the model (if it used configs)
 checkpoint_dir: "/content/ncsnv2/exp/logs/celeba/checkpoint_210000.pth" #Path to pretrained model

data:
 data_path: "/content/ncsnv2/exp/datasets/celeba" #or 'ffhq'
 dataset: 'celeba'
 train_batch_size: 8
 val_batch_size: 16
 num_channels: 3
 num_train: 64
 num_val: 32
 num_test: 32

outer:
 meta_type: 'mle' #implicit, maml, mle
 auto_cond_log: true #whether to use autograd for the cond-log-likelihood and meta gradient computation w.r.t. x.
 maml_use_last: -1 #use only the last n iterations of inner optimization for maml. -1 means use all.
 save_inits: false #whether to save the last found optimal solution for a trainig image to initialize the next inner loop.   
 measurement_loss: false #whether the meta-loss should be on held-out measurements instead of true images
 meta_loss_type: 'l2' #what type of meta loss to use
 hyperparam_type: 'scalar' #type of hyperparam c to use. [scalar, vector, matrix] 
 num_iters: 25 #number of meta iterations to run
 optimizer: 'sgd' #meta optimizer. [adam, sgd]
 lr: 0.1 #meta learning rate
 lr_decay: 0.95 #exponential weight decay applied if validation loss is rising. -1 means no decay
 verbose: true #whether to print during execution. TODO supplant this with command line argument.
 debug: false #debug mode doesn't save anything
 val_iters: 1 #validate every n iterations
 checkpoint_iters: 5 #checkpoint every n iterations
 batches_per_iter: 4 #number of training batches to use per iteration. -1 means all batches 
 cg_iters: 25 #number of conjugate gradient iterations to run if using meta_type=implicit. 0 returns b as the solution. A usual value is 25.
 cg_verbose: 5 #print from conjugate gradient every n iterations. 0 means don't print
 cg_tol: 0.00001 #tolerance parameter for conjugate gradient residual. A usual value is 0.00001.
 cg_damping: 1 #damping parameter for conjugate gradient. Turns problem from Ax = b --> (cg_damping*I + A)x = b. Conditions Hessian. 0 means no damping
 finite_difference: false #whether to use finite difference for Hessian-vector product calculations. Automatically true if meta_type=hessian-free  
 finite_difference_coeff: 0.00000001 #value of r to use in finite difference. 0.00000001 is usual val
 ROI: false #[False, True, ((h_offset, w_offset),(h, w))] - whether to use an ROI

inner:
 alg: 'map' #type of inner algorithm. ['map', 'langevin']
 T: 5 #number of inner optimization steps. For langevin, number of steps per noise level. (5 for celeba, 3 for ffhq)
 decimation_factor: 5 #for langevin, factor to divide the number of noise levels by. 0 means don't decimate.
 decimation_type: 'linear' #['linear', 'log_last', 'log_first', 'last', 'first']
 verbose: 100 #print every n iterations. 0 means don't print
 lr: 0.0000033 #learning rate. (0.0000033 for celeba, 0.0000009 for ffhq for ncsnv2)

problem:
 measurement_type: 'identity' #measurement type ['superres', 'inpaint', 'identity', 'gaussian', 'circulant']
 num_measurements: 5000 #number of measurements for gaussian and circulant
 downsample_factor: 8 #factor to downsample each side of the image
 inpaint_size: 20 #masked region side length for inpainting
 add_noise: false #whether to add noise after measuring
 noise_type: "gaussian" #[gaussian, uniform] 
 noise_std: 1 
 add_dependent_noise: false #whether to add noise before measuring
 dependent_noise_type: "gaussian" #[gaussian, uniform]
 dependent_noise_std: 1
