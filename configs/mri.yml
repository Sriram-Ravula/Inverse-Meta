use_gpu: true #whether to use cuda
gpu_num: 0 #the GPU number to use. default is 0.
save_dir: "" #root of folder to save run information (format: save_dir/datasets, save_dir/logs, save_dir/model_weights)
seed: 2022
resume: false #whether to resume from a checkpoint
resume_dir: "" #the root of the checkpoint (log) folder to resume from

net:
 model: 'ncsnv2'
 config_file: "ncsnv2/configs/mri.yml" #Path to a config file for the model (if it used configs)
 checkpoint_dir: "ncsnv2-mri-mvue/logs/mri-mvue/checkpoint_100000.pth" #Path to pretrained model


data:
 data_path: "validation_cplx_scans_small/raw" #or 'FFHQ' (all caps)
 input_dir: "validation_cplx_scans_small/raw" #or 'FFHQ' (all caps)
 maps_dir: "validation_cplx_scans_small/maps_768" #or 'FFHQ' (all caps)
 dataset: 'mri'
 train_batch_size: 1
 val_batch_size: 1
 num_channels: 2
 num_train: 1
 num_val: 1
 num_test: 1

outer:
 meta_type: 'mle' #implicit, maml, mle
 use_autograd: false #whether to use autograd for the cond-log-likelihood and meta gradient computation w.r.t. x.
 maml_use_last: -1 #use only the last n iterations of inner optimization for maml. -1 means use all.
 save_inits: true #whether to save the last found optimal solution for a trainig image to initialize the next inner loop.
 measurement_loss: false #whether the meta-loss should be on held-out measurements instead of true images
 meta_loss_type: 'l2' #what type of meta loss to use
 hyperparam_type: 'vector' #type of hyperparam c to use. [scalar, vector, matrix]
 hyperparam_init: 1 #the value to initialize the hyperparameter at. Default is 1
 num_iters: 25 #number of meta iterations to run
 optimizer: 'adam' #meta optimizer. [adam, sgd, linesearch]
 lr: 100 #meta learning rate
 lr_decay: 0.5 #exponential weight decay applied if validation loss is rising. -1 or false means no decay
 decay_on_val: false #if true, uses validation loss for decay; otherwise will decay every training iteration
 verbose: true #whether to print during execution.
 plot_imgs: true #whether to plot the data and reconstructions inline. set false if not in a notebook.
 debug: false #debug mode doesn't save anything.
 use_validation: true #if set to true, will use a validation set.
 val_iters: 1 #validate every n iterations
 checkpoint_iters: 1 #checkpoint every n iterations
 batches_per_iter: 2 #number of training batches to use per iteration. -1 means all batches
 cg_iters: 25 #number of conjugate gradient iterations to run if using meta_type=implicit. 0 returns b as the solution. A usual value is 25.
 cg_verbose: 5 #print from conjugate gradient every n iterations. 0 means don't print
 cg_tol: 0.00001 #tolerance parameter for conjugate gradient residual. A usual value is 0.00001.
 cg_damping: 1 #damping parameter for conjugate gradient. Turns problem from Ax = b --> (cg_damping*I + A)x = b. Conditions Hessian. 0 means no damping
 finite_difference: false #whether to use finite difference for Hessian-vector product calculations. Automatically true if meta_type=hessian-free
 finite_difference_coeff: 0.00000001 #value of r to use in finite difference. 0.00000001 is usual val
 ROI: false #[False, True, ((h_offset, w_offset),(h, w))] - whether to use an ROI

inner:
 alg: 'map' #type of inner algorithm. ['map', 'langevin']
 T: 3 #number of inner optimization steps. For langevin, number of steps per noise level. (5 for celeba, 3 for ffhq)
 decimation_factor: 3 #for langevin, factor to divide the number of noise levels by. 0 means don't decimate.
 decimation_type: 'linear' #['linear', 'log_last', 'log_first', 'last', 'first']
 verbose: 100 #print every n iterations. 0 means don't print
 lr: 0.00005 #learning rate. (0.0000033 for celeba, 0.0000009 for ffhq for ncsnv2)

problem:
 measurement_type: 'mri' #measurement type ['superres', 'inpaint', 'identity', 'gaussian', 'circulant']
 add_noise: false
 add_dependent_noise: false #whether to add noise before measuring
 R: 4
 orientation: 'vertical'
 pattern: 'equispaced'
